{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ae529e0-d397-40e8-9849-a2ff50a29cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import re\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "from itertools import permutations\n",
    "\n",
    "from pyphonetics import RefinedSoundex, FuzzySoundex\n",
    "from phonetics import dmetaphone, nysiis\n",
    "from textdistance import Levenshtein, DamerauLevenshtein as DL, JaroWinkler as JW, SmithWaterman as SW, NeedlemanWunsch as NW\n",
    "from textdistance import Jaccard, Sorensen, Overlap, Bag\n",
    "from textdistance import Editex\n",
    "from textdistance import LCSSeq, LCSStr\n",
    "from textdistance import BZ2NCD, ZLIBNCD\n",
    "\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04c19289-f660-4d10-a6ee-57d41ae4f844",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read files\n",
    "with open('titles.txt') as f:\n",
    "    titles = f.read()\n",
    "titles = titles.split()\n",
    "\n",
    "df_train_match = pd.read_csv('data/train_names_matches.csv', index_col=None)\n",
    "df_train_mismatch = pd.read_csv('data/train_names_mismatches.csv', index_col=[0])\n",
    "df_val_match = pd.read_csv('data/val_names_matches.csv', index_col=None)\n",
    "df_val_mismatch = pd.read_csv('data/val_names_mismatches.csv', index_col=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fe88e3-2646-40ca-852a-bb573d176c42",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "465c2777-83f4-4736-8a1f-756d05dfaf6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 59120/59120 [00:01<00:00, 41986.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 104876/104876 [00:02<00:00, 42814.89it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 14150/14150 [00:00<00:00, 38991.71it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 19416/19416 [00:00<00:00, 44018.76it/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_names(names):\n",
    "    preprocessed_names = []\n",
    "    for name in names:\n",
    "        name = str(name).lower().strip()\n",
    "        name = ' '.join(name.split())\n",
    "        name = unidecode(name)\n",
    "        name = re.sub('[^a-z ]', '', name)\n",
    "        name = ' '.join([part for part in name.split() if part not in titles])\n",
    "        preprocessed_names.append(name)\n",
    "    return preprocessed_names\n",
    "\n",
    "df_train_match[['name1', 'name2']] = df_train_match[['name1', 'name2']].progress_apply(lambda vec: preprocess_names(vec), axis=1, result_type='expand')\n",
    "df_train_mismatch[['name1', 'name2']] = df_train_mismatch[['name1', 'name2']].progress_apply(lambda vec: preprocess_names(vec), axis=1, result_type='expand')\n",
    "df_val_match[['name1', 'name2']] = df_val_match[['name1', 'name2']].progress_apply(lambda vec: preprocess_names(vec), axis=1, result_type='expand')\n",
    "df_val_mismatch[['name1', 'name2']] = df_val_mismatch[['name1', 'name2']].progress_apply(lambda vec: preprocess_names(vec), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bb14f7e-311c-4f83-bd7b-772dfaf35a98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_name(name, qval):\n",
    "    name = ''.join((['<']*(qval-1))) + name + ''.join((['>']*(qval-1)))\n",
    "    return name\n",
    "\n",
    "def get_similarity(names, sim_func, **kwargs):\n",
    "    if 'qval' in kwargs:\n",
    "        names = [pad_name(name, kwargs.get('qval', 1)) for name in names]\n",
    "    return sim_func(*names, **kwargs)\n",
    "\n",
    "# phonetic\n",
    "def soundex(name1, name2):\n",
    "    rs = RefinedSoundex()\n",
    "    return 1 - rs.distance(name1, name2, metric='levenshtein')/max(len(name1), len(name2))\n",
    "\n",
    "def double_metaphone(name1, name2):\n",
    "    p1 = set([p for p in dmetaphone(name1) if p!=''])\n",
    "    p2 = set([p for p in dmetaphone(name2) if p!=''])\n",
    "    return len(p1.intersection(p2))/len(p1.union(p2))\n",
    "\n",
    "# token based\n",
    "def jaccard(name1, name2, qval):\n",
    "    jc = Jaccard(qval)\n",
    "    return jc(name1, name2)\n",
    "\n",
    "def sorenson(name1, name2, qval):\n",
    "    sn = Sorensen(qval)\n",
    "    return sn(name1, name2)\n",
    "\n",
    "def overlap(name1, name2, qval):\n",
    "    ov = Overlap(qval)\n",
    "    return ov(name1, name2)\n",
    "\n",
    "def bag(name1, name2):\n",
    "    bag = Bag()\n",
    "    return 1 - bag(name1, name2)/max(len(name1), len(name2))\n",
    "\n",
    "# edit distance\n",
    "def levenshtein(name1, name2):\n",
    "    lev = Levenshtein()\n",
    "    return 1 - lev(name1, name2)/max(len(name1), len(name2))\n",
    "\n",
    "def dlevenshtein(name1, name2):\n",
    "    dl = DL()\n",
    "    return 1 - dl(name1, name2)/max(len(name1), len(name2))\n",
    "\n",
    "# alignment scores\n",
    "def smith_waterman(name1, name2):\n",
    "    sw = SW()\n",
    "    return sw(name1, name2)/max(len(name1), len(name2))\n",
    "\n",
    "def needleman_wunsch(name1, name2):\n",
    "    nw = NW()\n",
    "    return nw(name1, name2)/max(len(name1), len(name2))\n",
    "\n",
    "def jaro_winkler(name1, name2):\n",
    "    jw = JW()\n",
    "    return jw(name1, name2)\n",
    "\n",
    "# combined\n",
    "def editex(name1, name2):\n",
    "    ed = Editex()\n",
    "    return 1 - ed(name1, name2)/(2*max(len(name1), len(name2)))\n",
    "\n",
    "#sequence based\n",
    "def lcsseq(name1, name2):\n",
    "    lsq = LCSSeq()\n",
    "    return len(lsq(name1, name2))/max(len(name1), len(name2))\n",
    "\n",
    "def lcsstr(name1, name2):\n",
    "    lsr = LCSStr()\n",
    "    return len(lsr(name1, name2))/max(len(name1), len(name2))\n",
    "\n",
    "# compression based\n",
    "def bz2ncd(name1, name2):\n",
    "    bzn = BZ2NCD()\n",
    "    return 1 - bzn(name1, name2)\n",
    "\n",
    "def zlibncd(name1, name2):\n",
    "    zzn = ZLIBNCD()\n",
    "    return 1 - zzn(name1, name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4baa24e1-3c8a-4f8b-8e47-f7b96cfcb98e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_data(df_match, df_mismatch, count):\n",
    "    df_match = df_match[(df_match['name1']!=df_match['name2'])&(df_match['name1']!='')&(df_match['name2']!='')]\n",
    "    df_match = df_match.reset_index(drop=True).sample(n=count//2, random_state=42)\n",
    "    df_mismatch = df_mismatch.rename(columns={'mismatch': 'variations'})\n",
    "    df_mismatch = df_mismatch[(df_mismatch['name1']!=df_mismatch['name2'])&(df_mismatch['name1']!='')&(df_mismatch['name2']!='')]\n",
    "    df_mismatch = df_mismatch.reset_index(drop=True).sample(n=count//2, random_state=42)\n",
    "    df_match['label'] = 1\n",
    "    df_mismatch['label'] = 0\n",
    "    df = pd.concat([df_match, df_mismatch], axis=0).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df_train = sample_data(df_train_match, df_train_mismatch, 20000)\n",
    "df_val = sample_data(df_val_match, df_val_mismatch, 6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1104578-5ea0-4317-9d00-323f6e77c90f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 20000/20000 [00:36<00:00, 541.06it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 6000/6000 [00:09<00:00, 614.90it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_permutations(name):\n",
    "    name_permutations = [' '.join(parts_of_name) for parts_of_name in set(permutations(name.split()))]\n",
    "    return name_permutations\n",
    "\n",
    "def get_alignments(names):\n",
    "    name1_permutations = get_permutations(names[0])\n",
    "    name2_permutations = get_permutations(names[1])\n",
    "    alignment_budget = 10\n",
    "    n_alignments_name1 = int(np.ceil(alignment_budget*len(name1_permutations)/(len(name1_permutations)+len(name2_permutations))))\n",
    "    n_alignments_name2 = int(np.ceil(alignment_budget*len(name2_permutations)/(len(name1_permutations)+len(name2_permutations))))\n",
    "    return name1_permutations[0:n_alignments_name1], name2_permutations[0:n_alignments_name2]\n",
    "\n",
    "def get_best_alignment(names):\n",
    "    alignments_name1, alignments_name2 = get_alignments(names)\n",
    "    name_pairs = list(zip(*[arr.flatten() for arr in np.meshgrid(alignments_name1, alignments_name2)]))\n",
    "    name_pairs_sorted = sorted(name_pairs, key=lambda vec: needleman_wunsch(vec[0], vec[1]), reverse=True)\n",
    "    return name_pairs_sorted[0]\n",
    "\n",
    "df_train[['name1', 'name2']] = df_train[['name1', 'name2']].progress_apply(get_best_alignment, axis=1, result_type='expand')\n",
    "df_val[['name1', 'name2']] = df_val[['name1', 'name2']].progress_apply(get_best_alignment, axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b219451a-8730-4330-95a5-8145f27fa939",
   "metadata": {},
   "source": [
    "### Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "740e5629-1866-45a3-b458-c6dcc41549a0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 20000/20000 [00:01<00:00, 15565.25it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 26438.17it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 23312.67it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 23499.54it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 27678.15it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 26130.11it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 29273.27it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 26025.07it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 33087.41it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 80741.99it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 85611.76it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 94838.83it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 20000/20000 [00:04<00:00, 4057.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 20000/20000 [00:41<00:00, 478.91it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 20000/20000 [00:02<00:00, 6694.83it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 50366.18it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 20000/20000 [00:03<00:00, 5540.96it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 28215.32it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6000/6000 [00:00<00:00, 16310.66it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6000/6000 [00:00<00:00, 28542.36it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6000/6000 [00:00<00:00, 25392.09it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6000/6000 [00:00<00:00, 22474.36it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6000/6000 [00:00<00:00, 27726.26it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6000/6000 [00:00<00:00, 24844.34it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6000/6000 [00:00<00:00, 28775.08it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6000/6000 [00:00<00:00, 25875.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6000/6000 [00:00<00:00, 30436.22it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6000/6000 [00:00<00:00, 87815.53it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6000/6000 [00:00<00:00, 88633.59it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6000/6000 [00:00<00:00, 94467.37it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6000/6000 [00:01<00:00, 4236.60it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 6000/6000 [00:11<00:00, 533.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6000/6000 [00:00<00:00, 6424.82it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6000/6000 [00:00<00:00, 46630.41it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6000/6000 [00:01<00:00, 5233.86it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 6000/6000 [00:00<00:00, 29358.00it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_features(df):\n",
    "    df['soundex'] = df[['name1', 'name2']].progress_apply(get_similarity, args=(soundex,), axis=1)\n",
    "    df['double_metaphone'] = df[['name1', 'name2']].progress_apply(get_similarity, args=(double_metaphone,), axis=1)\n",
    "    df['jaccard_2'] = df[['name1', 'name2']].progress_apply(get_similarity, args=(jaccard,), qval=2, axis=1)\n",
    "    df['jaccard_3'] = df[['name1', 'name2']].progress_apply(get_similarity, args=(jaccard,), qval=3, axis=1)\n",
    "    df['sorenson_2'] = df[['name1', 'name2']].progress_apply(get_similarity, args=(sorenson,), qval=2, axis=1)\n",
    "    df['sorenson_3'] = df[['name1', 'name2']].progress_apply(get_similarity, args=(sorenson,), qval=3, axis=1)\n",
    "    df['overlap_2'] = df[['name1', 'name2']].progress_apply(get_similarity, args=(overlap,), qval=2, axis=1)\n",
    "    df['overlap_3'] = df[['name1', 'name2']].progress_apply(get_similarity, args=(overlap,), qval=3, axis=1)\n",
    "    df['bag'] = df[['name1', 'name2']].progress_apply(get_similarity, args=(bag,), axis=1)\n",
    "    df['levenshtein'] = df[['name1', 'name2']].progress_apply(get_similarity, args=(levenshtein,), axis=1)\n",
    "    df['dlevenshtein'] = df[['name1', 'name2']].progress_apply(get_similarity, args=(dlevenshtein,), axis=1)\n",
    "    df['jaro_winkler'] = df[['name1', 'name2']].progress_apply(get_similarity, args=(jaro_winkler,), axis=1)\n",
    "    df['smith_waterman'] = df[['name1', 'name2']].progress_apply(get_similarity, args=(smith_waterman,), axis=1)\n",
    "    df['editex'] = df[['name1', 'name2']].progress_apply(get_similarity, args=(editex,), axis=1)\n",
    "    df['lcsseq'] = df[['name1', 'name2']].progress_apply(get_similarity, args=(lcsseq,), axis=1)\n",
    "    df['lcsstr'] = df[['name1', 'name2']].progress_apply(get_similarity, args=(lcsstr,), axis=1)\n",
    "    df['bz2ncd'] = df[['name1', 'name2']].progress_apply(get_similarity, args=(bz2ncd,), axis=1)\n",
    "    df['zlibncd'] = df[['name1', 'name2']].progress_apply(get_similarity, args=(zlibncd,), axis=1)\n",
    "    return df\n",
    "\n",
    "df_train = generate_features(df_train)\n",
    "df_val = generate_features(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46c60d83-f10f-4eaf-bcb9-cc429d24afb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.to_csv('data/df_train.csv')\n",
    "df_val.to_csv('data/df_val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb0999d-8c9e-4db3-985f-f2703fae7dd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Feature Analysis\n",
    "1. Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e13575-de69-4aa8-bd66-d22b96b2b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_measures = ['soundex', 'double_metaphone', 'jaccard_2', 'jaccard_3', 'sorenson_2', 'sorenson_3', 'overlap_2', 'overlap_3', 'bag', \\\n",
    "                       'levenshtein', 'dlevenshtein', 'jaro_winkler', 'smith_waterman', 'editex', 'lcsseq', 'lcsstr', 'bz2ncd', 'zlibncd']\n",
    "variations = ['ocr_kte', 'pen', 'wm', 'ocr', 'wj', 'ab_kte', 'alt_tse', 'alt', 'kte', 'se', 'tse_kte', 'wm_kte',\\\n",
    "              'ab', 'se_kte', 'pen_kte', 'wj_kte', 'he', 'alt_kte', 'tse', 'ocr_tse', 'he_kte']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "480eea6c-711a-4f2e-bfb4-40ada57efdfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame({})\n",
    "df_mismatch = df_train[df_train['label']==0].reset_index(drop=True)\n",
    "for var in variations:\n",
    "    for sim in similarity_measures:\n",
    "        data = pd.concat([df_train[df_train['variations']==var], df_mismatch], ignore_index=True)\n",
    "        df_results.loc[sim, var] = average_precision_score(data['label'], data[sim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2935d31b-2b79-4eae-a890-22725238f341",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_results.to_csv('output/univariate_analysis_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3f6726-f5f6-498c-af36-96d2dae2d83e",
   "metadata": {},
   "source": [
    "2. Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "330aa0e9-c71d-45dd-a89c-e7064f12693a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_corr = df_train[similarity_measures].corr()\n",
    "df_corr.to_csv('output/correlation_train.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mw)",
   "language": "python",
   "name": "mw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
